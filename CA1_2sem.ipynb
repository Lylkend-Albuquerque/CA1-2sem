{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549a131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/24 22:04:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Occupancy Estimation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a94b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Occupancy_Estimation.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema to understand the data types\n",
    "df.printSchema()\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a90442",
   "metadata": {},
   "source": [
    "#Features and Target\n",
    "\n",
    " - Target is predict Room Ocuppancy which is(Room_Occupancy_Count column)\n",
    " - All the sensors readings would be the features to be analysed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e10c65a",
   "metadata": {},
   "source": [
    "The follow function \"create_sequences\" it's because of error during the sequence creation step, the dataset size does not align well with the specified time_steps. As was created sequences with a fixed number of time steps, It's needs to ensure that the loop or indexing logic does not attempt to access data beyond the last available index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7053242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        # Make sure not to exceed the bounds\n",
    "        end_ix = i + time_steps\n",
    "        if end_ix > len(X):\n",
    "            break  # Break the loop if the end index exceeds the dataset size\n",
    "        seq_X = X[i:end_ix]\n",
    "        seq_y = y[end_ix - 1]  # Target is the last value in the sequence\n",
    "        Xs.append(seq_X)\n",
    "        ys.append(seq_y)\n",
    "    return np.array(Xs), np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb01e609",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Converting X and y inputs into NumPy arrays before passing them to the function \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# as the function uses array slicing which is more straightforward with NumPy arrays than\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# with pandas Series or DataFrames.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train_seq, y_train_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_train\u001b[38;5;241m.\u001b[39mvalues, y_train\u001b[38;5;241m.\u001b[39mvalues, time_steps)\n\u001b[1;32m      6\u001b[0m X_test_seq, y_test_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_test\u001b[38;5;241m.\u001b[39mvalues, y_test\u001b[38;5;241m.\u001b[39mvalues, time_steps)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# Converting X and y inputs into NumPy arrays before passing them to the function \n",
    "# as the function uses array slicing which is more straightforward with NumPy arrays than\n",
    "# with pandas Series or DataFrames.\n",
    "\n",
    "# X_train_seq, y_train_seq = create_sequences(X_train.values, y_train.values, time_steps)\n",
    "# X_test_seq, y_test_seq = create_sequences(X_test.values, y_test.values, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c58914c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "61",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 61",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(Xs), np\u001b[38;5;241m.\u001b[39marray(ys)\n\u001b[1;32m     24\u001b[0m time_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m---> 25\u001b[0m X_train_seq, y_train_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_train, y_train, time_steps)\n\u001b[1;32m     26\u001b[0m X_test_seq, y_test_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_test, y_test, time_steps)\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[0;34m(X, y, time_steps)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m-\u001b[39m time_steps):\n\u001b[1;32m     20\u001b[0m     Xs\u001b[38;5;241m.\u001b[39mappend(X[i:(i \u001b[38;5;241m+\u001b[39m time_steps)])\n\u001b[0;32m---> 21\u001b[0m     ys\u001b[38;5;241m.\u001b[39mappend(y[i \u001b[38;5;241m+\u001b[39m time_steps])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(Xs), np\u001b[38;5;241m.\u001b[39marray(ys)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 61"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "df1 = pd.read_csv('Occupancy_Estimation.csv')\n",
    "\n",
    "# Exclude 'Date', 'Time', and the target column for features\n",
    "features = df1.drop(['Date', 'Time', 'Room_Occupancy_Count'], axis=1)\n",
    "target = df1['Room_Occupancy_Count']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data for LSTM [samples, time steps, features]\n",
    "# This function will convert the data into sequences\n",
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 60\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2161b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding 'Date', 'Time', and 'Room_Occupancy_Count' - Those are not need features\n",
    "# I could have set just 16 as my Data set has 19 features but as good practice \n",
    "# If I get a large data set no need to count the number of features with this code in case of recycling code.\n",
    "\n",
    "number_of_features = len(df.columns) - 3  # Adjusted for Date, Time, and target column\n",
    "time_steps = 60\n",
    "input_shape = (time_steps, number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03386d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Example model structure\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(input_shape)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbcb91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8103, 16) (8103,) (2026, 16) (2026,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "003108ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    max_start_index = len(X) - time_steps\n",
    "    if max_start_index < 0:\n",
    "        raise ValueError(\"The dataset is too small for the number of time steps specified.\")\n",
    "    for i in range(max_start_index + 1):  # Ensure we do not go beyond the dataset\n",
    "        seq_X = X[i:(i + time_steps)]\n",
    "        seq_y = y[i + time_steps - 1]  # Adjusted to ensure it's within bounds\n",
    "        Xs.append(seq_X)\n",
    "        ys.append(seq_y)\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e319cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Time', 'S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Light',\n",
      "       'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound',\n",
      "       'S4_Sound', 'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR',\n",
      "       'Room_Occupancy_Count'],\n",
      "      dtype='object')\n",
      "RangeIndex(start=0, stop=10129, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)  # For DataFrames\n",
    "print(df1.index)    # For checking indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939fc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
