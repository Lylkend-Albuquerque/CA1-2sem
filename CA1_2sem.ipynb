{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3ba179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 23:21:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Occupancy Estimation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a94b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset from HDFS - Biga data Tecnology to storage\n",
    "df = spark.read.csv(\"hdfs:///user1/Occupancy_Estimation.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69fe162",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "    - as EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345cc215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- S1_Temp: double (nullable = true)\n",
      " |-- S2_Temp: double (nullable = true)\n",
      " |-- S3_Temp: double (nullable = true)\n",
      " |-- S4_Temp: double (nullable = true)\n",
      " |-- S1_Light: integer (nullable = true)\n",
      " |-- S2_Light: integer (nullable = true)\n",
      " |-- S3_Light: integer (nullable = true)\n",
      " |-- S4_Light: integer (nullable = true)\n",
      " |-- S1_Sound: double (nullable = true)\n",
      " |-- S2_Sound: double (nullable = true)\n",
      " |-- S3_Sound: double (nullable = true)\n",
      " |-- S4_Sound: double (nullable = true)\n",
      " |-- S5_CO2: integer (nullable = true)\n",
      " |-- S5_CO2_Slope: double (nullable = true)\n",
      " |-- S6_PIR: integer (nullable = true)\n",
      " |-- S7_PIR: integer (nullable = true)\n",
      " |-- Room_Occupancy_Count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 23:21:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|summary|      Date|           S1_Temp|           S2_Temp|            S3_Temp|           S4_Temp|          S1_Light|          S2_Light|         S3_Light|         S4_Light|           S1_Sound|           S2_Sound|           S3_Sound|           S4_Sound|            S5_CO2|        S5_CO2_Slope|             S6_PIR|             S7_PIR|Room_Occupancy_Count|\n",
      "+-------+----------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|  count|     10129|             10129|             10129|              10129|             10129|             10129|             10129|            10129|            10129|              10129|              10129|              10129|              10129|             10129|               10129|              10129|              10129|               10129|\n",
      "|   mean|      null|25.454012242078203| 25.54605884095266| 25.056620594333857|25.754124790207086|25.445058742225292|26.016289860795734|34.24849442195676|13.22025866324415|0.16817751011942533|0.12006614670744918|0.15811926152627273|0.10384045809063676|  460.860400829302|-0.00483000068350229|0.09013722973640044|0.07957350182643894|  0.3985585941356501|\n",
      "| stddev|      null| 0.351350551388067|0.5863254509673754|0.42728250832189574| 0.356434071686561| 51.01126404586223| 67.30417025816692|58.40074377922952|19.60221922917804| 0.3167091032568891|  0.266502502233752| 0.4136366134889066|0.12068279538984712|199.96493980975043|  1.1649895813031426| 0.2863924006559917| 0.2706451389275408|  0.8936330738301977|\n",
      "|    min|2017/12/22|             24.94|             24.75|              24.44|             24.94|                 0|                 0|                0|                0|               0.06|               0.04|               0.04|               0.05|               345|      -6.29615384615|                  0|                  0|                   0|\n",
      "|    max|2018/01/11|             26.38|              29.0|              26.19|             26.56|               165|               258|              280|               74|               3.88|               3.44|               3.67|                3.4|              1270|       8.98076923077|                  1|                  1|                   3|\n",
      "+-------+----------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to understand the data types\n",
    "df.printSchema()\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374facd0",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset with PySpark \n",
    "    \n",
    "     - Checking for Missing Values\n",
    "     - Cleaning features that won't be used by the LSTM --> Feature Engineering\n",
    "     - Converting to a Pandas Df to preprocecing\n",
    "     - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3309ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+------+------------+------+------+--------------------+\n",
      "| Date| Time|S1_Temp|S2_Temp|S3_Temp|S4_Temp|S1_Light|S2_Light|S3_Light|S4_Light|S1_Sound|S2_Sound|S3_Sound|S4_Sound|S5_CO2|S5_CO2_Slope|S6_PIR|S7_PIR|Room_Occupancy_Count|\n",
      "+-----+-----+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+------+------------+------+------+--------------------+\n",
      "|10129|10129|  10129|  10129|  10129|  10129|   10129|   10129|   10129|   10129|   10129|   10129|   10129|   10129| 10129|       10129| 10129| 10129|               10129|\n",
      "+-----+-----+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+------+------------+------+------+--------------------+\n",
      "\n",
      "Missing values in each column:\n",
      " Date                    0\n",
      "Time                    0\n",
      "S1_Temp                 0\n",
      "S2_Temp                 0\n",
      "S3_Temp                 0\n",
      "S4_Temp                 0\n",
      "S1_Light                0\n",
      "S2_Light                0\n",
      "S3_Light                0\n",
      "S4_Light                0\n",
      "S1_Sound                0\n",
      "S2_Sound                0\n",
      "S3_Sound                0\n",
      "S4_Sound                0\n",
      "S5_CO2                  0\n",
      "S5_CO2_Slope            0\n",
      "S6_PIR                  0\n",
      "S7_PIR                  0\n",
      "Room_Occupancy_Count    0\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Count the total number of entries\n",
    "total_entries = df.count()\n",
    "\n",
    "# Count the number of non-null (non-missing) values in each column\n",
    "non_missing_counts = df.agg(*(count(c).alias(c) for c in df.columns))\n",
    "\n",
    "# To view the results\n",
    "non_missing_counts.show()\n",
    "\n",
    "# Calculating missing values by subtracting non-missing from total entries\n",
    "# This involves converting the DataFrame to a Pandas DataFrame for easier manipulation\n",
    "non_missing_counts_pandas = non_missing_counts.toPandas()\n",
    "missing_values = total_entries - non_missing_counts_pandas.iloc[0]\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a90442",
   "metadata": {},
   "source": [
    "# Features and Target\n",
    "\n",
    " - Target is predict Room Ocuppancy which is(Room_Occupancy_Count)\n",
    " - All the sensors readings would be the features to be analysed\n",
    " - Let's perform a better Feature Engineering - separating onto differents df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db84bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and target variable\n",
    "feature_columns = [col for col in df.columns if col not in {'Date', 'Time', 'Room_Occupancy_Count'}]\n",
    "features_df = df.select(*feature_columns)\n",
    "target_df = df.select('Room_Occupancy_Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e10fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df and target_df are a Spark DataFrames also, convert those to Pandas for normalization(Features)\n",
    "pandas_features_df = features_df.toPandas()\n",
    "pandas_target_df = target_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a866e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler for features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "# Fit and transform the features\n",
    "pandas_features_df_scaled = pd.DataFrame(scaler_features.fit_transform(pandas_features_df),\n",
    "                                         columns=pandas_features_df.columns)\n",
    "\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "# As the target Df is a single column and needs to be reshaped for scaling\n",
    "pandas_target_df_scaled = scaler_target.fit_transform(pandas_target_df.values.reshape(-1, 1))\n",
    "# Flatten back to a series maintaining the original index\n",
    "pandas_target_df_scaled = pd.Series(pandas_target_df_scaled.flatten(), index=pandas_target_df.index)\n",
    "\n",
    "# pandas_features_df_scaled -> Normalized features\n",
    "# pandas_target_df_scaled -> Normalized target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca5e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting time series data for LSTM models, ensuring the split preserves the sequence's temporal order\n",
    "# Determine the split point for an 80/20 train-test split\n",
    "split_point = int(len(pandas_features_df_scaled) * 0.8)\n",
    "\n",
    "# Splitting the features\n",
    "X_train = pandas_features_df_scaled.iloc[:split_point]\n",
    "X_test = pandas_features_df_scaled.iloc[split_point:]\n",
    "\n",
    "# Check if the target has been scaled and split accordingly\n",
    "# This checks if 'pandas_target_df_scaled' has been defined and uses it; if not, it falls back to 'pandas_target_df'\n",
    "if 'pandas_target_df_scaled' in locals():\n",
    "    y_train = pandas_target_df_scaled.iloc[:split_point]\n",
    "    y_test = pandas_target_df_scaled.iloc[split_point:]\n",
    "else:\n",
    "    y_train = pandas_target_df.iloc[:split_point]\n",
    "    y_test = pandas_target_df.iloc[split_point:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe027132",
   "metadata": {},
   "source": [
    "Inverse Transformation: as was normalized the target variable, keep the scaler_target object so it will needs later  to use scaler_target.inverse_transform() on top of predictions, in order to interpret the results in the original scale of the target variable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e10c65a",
   "metadata": {},
   "source": [
    "The follow function \"create_sequences\" it's because of error during the sequence creation step, the dataset size does not align well with the specified time_steps. As was created sequences with a fixed number of time steps, It's needs to ensure that the loop or indexing logic does not attempt to access data beyond the last available index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7053242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        # Make sure not to exceed the bounds\n",
    "        end_ix = i + time_steps\n",
    "        if end_ix > len(X):\n",
    "            break  # Break the loop if the end index exceeds the dataset size\n",
    "        seq_X = X[i:end_ix]\n",
    "        seq_y = y[end_ix - 1]  # Target is the last value in the sequence\n",
    "        Xs.append(seq_X)\n",
    "        ys.append(seq_y)\n",
    "    return np.array(Xs), np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb01e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X and y inputs into NumPy arrays before passing them to the function \n",
    "# as the function uses array slicing which is more straightforward with NumPy arrays than\n",
    "# with pandas Series or DataFrames.\n",
    "\n",
    "# X_train_seq, y_train_seq = create_sequences(X_train.values, y_train.values, time_steps)\n",
    "# X_test_seq, y_test_seq = create_sequences(X_test.values, y_test.values, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2161b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps=60)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbbcb91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8103, 16) (8103,) (2026, 16) (2026,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "003108ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e319cab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939fc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
